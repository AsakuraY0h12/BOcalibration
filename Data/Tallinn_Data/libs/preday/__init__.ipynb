{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in c:\\users\\dzerz\\anaconda3\\envs\\calibration_using_bo\\lib\\site-packages (1.10.1)\n",
      "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in c:\\users\\dzerz\\anaconda3\\envs\\calibration_using_bo\\lib\\site-packages (from scipy) (1.23.5)\n",
      "Requirement already satisfied: pyDOE in c:\\users\\dzerz\\anaconda3\\envs\\calibration_using_bo\\lib\\site-packages (0.3.8)\n",
      "Requirement already satisfied: numpy in c:\\users\\dzerz\\anaconda3\\envs\\calibration_using_bo\\lib\\site-packages (from pyDOE) (1.23.5)\n",
      "Requirement already satisfied: scipy in c:\\users\\dzerz\\anaconda3\\envs\\calibration_using_bo\\lib\\site-packages (from pyDOE) (1.10.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install scipy\n",
    "!pip install pyDOE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy.stats import pearsonr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import os\n",
    "import time\n",
    "from pyDOE import lhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function that execute system call to replace necessary line with param value\n",
    "# Params:\n",
    "# - model: model name (filename without extension)\n",
    "# - param_name: name of the parameter\n",
    "# - declaration: type of declaration used in the code (local or as property of an object)\n",
    "# - value: value to be set\n",
    "\n",
    "def fn_param_update(model, param_name, declaration, value):\n",
    "    \n",
    "    ## Get the global setting object\n",
    "    setting_obj = shared.env.settings\n",
    "    \n",
    "    if setting_obj.MOCKING_MODE:   ## Mocking mode is useful for local testing, not to run actual simulation!\n",
    "        return 0\n",
    "    \n",
    "    sys_call_param = [\n",
    "        \"sh\",\n",
    "        setting_obj.SH_UPDATE_PARAM,\n",
    "        os.path.join(setting_obj.PARAM_FILE_PATH, f\"{model}.{setting_obj.PARAM_FILE_EXTENSION}\"),\n",
    "        f\"'{param_name} = {declaration}{param_name}={value}'\"\n",
    "    ]\n",
    "    \n",
    "    if setting_obj.CONTAINERIZED:\n",
    "        sys_call_param += [\n",
    "            f\"{model}.{setting_obj.PARAM_FILE_EXTENSION}\",\n",
    "            os.path.join(setting_obj.CONTAINER_HOME_PATH, \"runtime\"),\n",
    "            setting_obj.FINGERPRINT\n",
    "        ]\n",
    "        \n",
    "    output = os.system(\" \".join(sys_call_param))\n",
    "    \n",
    "    if output is None or \"status\" not in output.__dict__:\n",
    "        return 0\n",
    "\n",
    "    \n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_param_push():\n",
    "    setting_obj = shared.env.settings\n",
    "    \n",
    "    if setting_obj.MOCKING_MODE:\n",
    "        return 0\n",
    "    \n",
    "    sys_call_param = [\n",
    "        \"sh\",\n",
    "        setting_obj.SH_PUSH_PARAM,\n",
    "        os.path.join(setting_obj.CONTAINER_HOME_PATH, \"runtime\"),\n",
    "        setting_obj.FINGERPRINT\n",
    "    ]\n",
    "    \n",
    "    output = os.system(\" \".join(sys_call_param))\n",
    "    \n",
    "    if output is None or \"status\" not in output.__dict__:\n",
    "        return 0\n",
    "    \n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function that will try to update all parameters based on a flag that signaling a change (flag \"changed\" T/F)\n",
    "## NOTE: DUring first run, if there are not existent model files, then \"changed\" flag need to be set to T for all parameters, otherwise part of them won't be set at all\n",
    "# Params:\n",
    "# - params: dataframe (tidy) containing necessary columns\n",
    "\n",
    "def fn_simulation_config(params):\n",
    "    # Get global settings object\n",
    "    setting_obj = shared.env.settings\n",
    "    \n",
    "    # Apply param updates and get status\n",
    "    status = []\n",
    "    for p in params:\n",
    "        status.append(fn_param_update(p['model'], p['param_name'], p['declaration'], p['param_name_left_hand'], p['value']))\n",
    "    \n",
    "    # Push params if containerized\n",
    "    if setting_obj.CONTAINERIZED and setting_obj.SH_PUSH_PARAM is not None:\n",
    "        push_status = fn_param_push()\n",
    "        if push_status != 0:\n",
    "            status = [push_status] * len(status)\n",
    "    \n",
    "    # Return status\n",
    "    return status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_worker_setup(setting_obj):\n",
    "    command = [\n",
    "        \"sh\",\n",
    "        setting_obj[\"SH_SETUP_WORKER\"],\n",
    "        setting_obj[\"CONTAINER_HOME_PATH\"],\n",
    "        os.path.sep.join([setting_obj[\"CONTAINER_HOME_PATH\"], \"runtime\"]),\n",
    "        setting_obj[\"FINGERPRINT\"],\n",
    "        setting_obj[\"IMAGE_NAME\"]\n",
    "    ]\n",
    "    output = subprocess.run(command, capture_output=True, text=True)\n",
    "    \n",
    "    if output.returncode is None:\n",
    "        return 0\n",
    "        \n",
    "    output.check_returncode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_worker_remove(setting_obj):\n",
    "    sys_call = [\"sh\", setting_obj['SH_REMOVE_WORKER'], \"/\".join([setting_obj['CONTAINER_HOME_PATH'], \"runtime\"]), setting_obj['FINGERPRINT']]\n",
    "    output = subprocess.run(sys_call, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    if output.returncode is None:\n",
    "        return 0\n",
    "    output.check_returncode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function that execute system call to run the simulation with SimMobility\n",
    "# Params:\n",
    "# - /\n",
    "\n",
    "def fn_simulation_call():\n",
    "    # Get global settings object\n",
    "    setting_obj = shared.env.settings\n",
    "    \n",
    "    if setting_obj.MOCKING_MODE:\n",
    "        return 0\n",
    "    \n",
    "    if os.path.exists(setting_obj.ACTIVITY_FILE):\n",
    "        os.remove(setting_obj.ACTIVITY_FILE)\n",
    "    \n",
    "    sys_call_param = [\n",
    "        \"sh\",\n",
    "        setting_obj.SH_EXEC_SIMULATION\n",
    "    ]\n",
    "    \n",
    "    if setting_obj.CONTAINERIZED:\n",
    "        sys_call_param.extend([os.path.join(setting_obj.CONTAINER_HOME_PATH, \"runtime\"), setting_obj.FINGERPRINT])\n",
    "    \n",
    "    output = os.system(sys_call_param)\n",
    "    \n",
    "    if os.path.exists(setting_obj.ACTIVITY_FILE):\n",
    "        return 0\n",
    "    \n",
    "    raise Exception(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function that calculates output statistics\n",
    "# Params:\n",
    "# - activity: activity_schedule table (dataframe,tible)\n",
    "# - pop_size: total population size [currently not in use as no ration of population is considered]\n",
    "\n",
    "def fn_output_stat(activity, pop_size=0):\n",
    "    if activity.empty:\n",
    "        return None\n",
    "    \n",
    "    pop_travelled = len(activity['person_id'].unique())\n",
    "    \n",
    "    activity['od'] = activity['prev_stop_location'] + '_' + activity['stop_location']\n",
    "    \n",
    "    stat_total = activity.groupby('person_id').agg({'tours': pd.Series.nunique, 'trips': 'count'}) \\\n",
    "        .agg({'tours': 'sum', 'trips': 'sum', 'avg_trips_tour': lambda x: round(x['trips']/x['tours'], 3)}) \\\n",
    "        .to_frame().transpose().rename(columns={'tours': 'tours_total', 'trips': 'trips_total', 'avg_trips_tour': 'avg_trips_tour_total'})\n",
    "    \n",
    "    stat_purpose = activity.groupby(['person_id', 'tourType']).agg({'tours': pd.Series.nunique, 'trips': 'count'}) \\\n",
    "        .groupby('tourType').agg({'tours': 'sum', 'trips': 'sum', 'avg_trips_tour': lambda x: round(x['trips']/x['tours'], 3)}) \\\n",
    "        .reset_index().rename(columns={'tourType': 'type'})\n",
    "    stat_purpose['scope'] = 'purpose'\n",
    "    \n",
    "    stat_mode = activity.groupby(['person_id', 'stop_mode']).agg({'tours': pd.Series.nunique, 'trips': 'count'}) \\\n",
    "        .groupby('stop_mode').agg({'tours': 'sum', 'trips': 'sum', 'avg_trips_tour': lambda x: round(x['trips']/x['tours'], 3)}) \\\n",
    "        .reset_index().rename(columns={'stop_mode': 'type'})\n",
    "    stat_mode['scope'] = 'mode'\n",
    "    \n",
    "    stat_total['type'] = 'general'\n",
    "    stat_total['scope'] = 'global'\n",
    "    \n",
    "    return pd.concat([stat_total.melt(id_vars=['type', 'scope'], var_name='variable', value_name='value'),\n",
    "                      stat_purpose.melt(id_vars=['type', 'scope'], var_name='variable', value_name='value'),\n",
    "                      stat_mode.melt(id_vars=['type', 'scope'], var_name='variable', value_name='value')]) \\\n",
    "             .filter(['scope', 'type', 'variable', 'value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### could do it same as the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_output_stat_virtualcity(activity, pop_size=0):\n",
    "    if activity.empty:\n",
    "        return None\n",
    "    \n",
    "    pop_travelled = activity.person_id.nunique()\n",
    "    \n",
    "    activity['od'] = activity['prev_stop_location'] + '_' + activity['stop_location']\n",
    "    \n",
    "    stat_total = (\n",
    "        activity.groupby('person_id')\n",
    "        .agg(\n",
    "            tours=('tour_no', 'nunique'),\n",
    "            stops=('od', 'nunique'),\n",
    "            trips=('person_id', 'count')\n",
    "        )\n",
    "        .sum()\n",
    "        .to_frame()\n",
    "        .T\n",
    "        .drop(columns=['person_id'])\n",
    "    )\n",
    "    \n",
    "    stat_purpose = (\n",
    "        activity.groupby(['person_id', 'tourType'])\n",
    "        .agg(\n",
    "            tours=('tour_no', 'nunique'),\n",
    "            stops=('od', 'nunique'),\n",
    "            trips=('person_id', 'count')\n",
    "        )\n",
    "        .groupby('tourType')\n",
    "        .agg(\n",
    "            tours=('tours', 'sum'),\n",
    "            stops=('stops', 'sum'),\n",
    "            trips=('trips', 'sum')\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "    \n",
    "    stat_purpose = pd.merge(\n",
    "        stat_purpose,\n",
    "        activity.groupby('tourType')\n",
    "        .agg(travel_ratio=('person_id', lambda x: x.nunique() / pop_travelled))\n",
    "        .reset_index(),\n",
    "        on='tourType'\n",
    "    )\n",
    "    \n",
    "    stat_mode = (\n",
    "        activity.groupby(['person_id', 'stop_mode'])\n",
    "        .agg(\n",
    "            tours=('tour_no', 'nunique'),\n",
    "            stops=('od', 'nunique'),\n",
    "            trips=('person_id', 'count')\n",
    "        )\n",
    "        .groupby('stop_mode')\n",
    "        .agg(\n",
    "            tours=('tours', 'sum'),\n",
    "            stops=('stops', 'sum'),\n",
    "            trips=('trips', 'sum')\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "    \n",
    "    stat_mode = pd.merge(\n",
    "        stat_mode,\n",
    "        activity.groupby('stop_mode')\n",
    "        .agg(travel_ratio=('person_id', lambda x: x.nunique() / pop_travelled))\n",
    "        .reset_index(),\n",
    "        on='stop_mode'\n",
    "    )\n",
    "    \n",
    "    return pd.concat([\n",
    "        pd.melt(stat_total.reset_index(), id_vars=['index'], var_name='variable', value_name='value')\n",
    "        .assign(scope='global', type='general')\n",
    "        .drop(columns=['index']),\n",
    "        pd.melt(stat_purpose, id_vars=['tourType'], var_name='variable', value_name='value')\n",
    "        .assign(scope='purpose'),\n",
    "        pd.melt(stat_mode, id_vars=['stop_mode'], var_name='variable', value_name='value')\n",
    "        .assign(scope='mode')\n",
    "        .rename(columns={'stop_mode': 'type'})\n",
    "    ], axis=0)[['scope', 'type', 'variable', 'value']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function that calculates output statistics\n",
    "# Params:\n",
    "# - activity: activity_schedule table (dataframe,tible)\n",
    "# - setting_obj: object with overall settings\n",
    "\n",
    "def fn_output_od(activity):\n",
    "    # Get global settings object\n",
    "    setting_obj = shared.env[\"settings\"]\n",
    "    \n",
    "    OD_cells = setting_obj[\"CITY_STATS\"][\"value\"].index.tolist()\n",
    "    OD_m = pd.DataFrame(0, index=OD_cells, columns=OD_cells)\n",
    "\n",
    "    if activity.empty:\n",
    "        return None\n",
    "\n",
    "    as_df = pd.merge(activity, setting_obj[\"CITY_STATS\"][\"district_map\"], \n",
    "                      left_on=\"prev_stop_location\", right_on=\"newTAZ\") \\\n",
    "             .merge(setting_obj[\"CITY_STATS\"][\"district_map\"], \n",
    "                    left_on=\"stop_location\", right_on=\"newTAZ\",\n",
    "                    suffixes=(\"_origin\", \"_destination\"))\n",
    "\n",
    "    for _, travel_subroute in as_df.iterrows():\n",
    "        origin_n = travel_subroute[\"code_origin\"]\n",
    "        destination_n = travel_subroute[\"code_destination\"]\n",
    "        OD_m.loc[origin_n, destination_n] += 1\n",
    "\n",
    "    return OD_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function that calculates output statistics (OD and balance between transportation modes)\n",
    "# Params:\n",
    "# - activity: activity_schedule table (dataframe,tible)\n",
    "# - setting_obj: object with overall settings\n",
    "\n",
    "def fn_output_od_mode_balance(activity):\n",
    "    # Get global settings object\n",
    "    setting_obj = shared.env.settings\n",
    "    \n",
    "    OD_m = setting_obj['CITY_STATS']['emptyOD']\n",
    "    output = {\n",
    "        'total_legs': len(activity),\n",
    "        'od': OD_m,\n",
    "        'balance': setting_obj['CITY_STATS']['balance'].assign(estimated_share=0).loc[:, ['mode_category', 'estimated_share', *setting_obj['CITY_STATS']['balance'].columns]],\n",
    "        'workers_work': 1\n",
    "    }\n",
    "    \n",
    "    if len(activity) == 0:\n",
    "        return output\n",
    "    \n",
    "    as_ = activity.merge(\n",
    "        setting_obj['CITY_STATS']['district_map'].loc[:, ['code', 'newTAZ']].rename(columns={'code': 'origin_station_code', 'newTAZ': 'prev_stop_location'}),\n",
    "        how='left', on='prev_stop_location'\n",
    "    ).merge(\n",
    "        setting_obj['CITY_STATS']['district_map'].loc[:, ['code', 'newTAZ']].rename(columns={'code': 'destination_station_code', 'newTAZ': 'stop_location'}),\n",
    "        how='left', on='stop_location'\n",
    "    ).loc[:, ['origin_station_code', 'destination_station_code']]\n",
    "    \n",
    "    for id_sr in range(len(as_)):\n",
    "        travel_subroute = as_.iloc[id_sr, :]\n",
    "        origin_n = travel_subroute['origin_station_code']\n",
    "        destination_n = travel_subroute['destination_station_code']\n",
    "        OD_m[origin_n, destination_n] += 1\n",
    "    \n",
    "    output['od'] = OD_m  # OD_m/sum(OD_m) ## IF OPERATES IN RELATIVE MODE\n",
    "    \n",
    "    # Transportation mode balance\n",
    "    if 'balance' in setting_obj['CITY_STATS']:\n",
    "        total_legs = len(activity)\n",
    "        mode_stats = activity.assign(\n",
    "            mode_category=lambda x: np.where(\n",
    "                x['stop_mode'].isin(['BusTravel', 'SMS']), 'public',\n",
    "                np.where(\n",
    "                    x['stop_mode'].isin(['Car', 'Car Sharing 2', 'Car Sharing 3']), 'car',\n",
    "                    np.where(x['stop_mode'].isin(['PrivateBus', 'Motorcycle', 'Taxi']), 'other', x['stop_mode'].str.lower())\n",
    "                )\n",
    "            )\n",
    "        ).groupby('mode_category')['mode_category'].agg([('estimated_share', lambda x: len(x) / total_legs)]).reset_index().merge(\n",
    "            setting_obj['CITY_STATS']['balance'], on='mode_category', how='right'\n",
    "        ).assign(estimated_share=lambda x: np.where(pd.isna(x['estimated_share']), 0, x['estimated_share']))\n",
    "        \n",
    "        output['balance'] = mode_stats.loc[:, ['mode_category', 'estimated_share', *setting_obj['CITY_STATS']['balance'].columns]]\n",
    "    \n",
    "    # Workers share scheduled to take a trip\n",
    "    if 'workers' in setting_obj['CITY_STATS']:\n",
    "        assigned_workers = activity.query(\"tourType == 'Work' and primary_stop == True\").assign(\n",
    "            person_id=lambda x: x['person_id'].astype(str)\n",
    "        ).assign(\n",
    "            id=lambda x: x['person_id'].str.split('-').str[0].astype(int)\n",
    "        ).query(\"id in @setting_obj['CITY_STATS']['workers']['id']\").loc[:, 'id'].unique()\n",
    "        \n",
    "        output['workers_work'] = 1 - (len(assigned_workers) / len(setting_obj['CITY_STATS']['workers']))\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Comparissons\n",
    "\n",
    "## Function that compare two vectors in terms of distance\n",
    "def fn_quantify_vector(vector_1, vector_2):                        # Euclidean\n",
    "    return np.sqrt(np.sum((vector_1 - vector_2)**2))\n",
    "\n",
    "def fn_quantify_vector_correlation(vector_1, vector_2):            # Pearson correlation\n",
    "    return pearsonr(vector_1, vector_2)[0]\n",
    "\n",
    "\n",
    "## Function that compare two matrix\n",
    "def fn_quantify_matrix(matrix_1, matrix_2, mat_weights=None, squared=False, absolute=False):\n",
    "    if mat_weights is None:\n",
    "        mat_dim = matrix_1.shape\n",
    "        mat_weights = np.ones((mat_dim[0], mat_dim[1]))\n",
    "    mat_difference = (matrix_1 - matrix_2)\n",
    "    if squared:\n",
    "        mat_difference = mat_difference**2\n",
    "    if absolute:\n",
    "        mat_difference = np.abs(mat_difference)\n",
    "    return fn_matrix_rmse(mat_difference)\n",
    "\n",
    "def fn_matrix_norm(mat):        # Frobenius matrix norm\n",
    "    return np.linalg.norm(mat)\n",
    "\n",
    "def fn_matrix_max_cell_loss(mat, observed_mat):\n",
    "    rel_mat = mat/observed_mat                                       ## max relative loss/error (loss per cell) - rounded at 4 decimals\n",
    "    rel_non_tol = len(np.where(rel_mat > 0.1)[0])/mat.shape[0]**2    ## rel number of cells with non-tollerable error\n",
    "    return round(np.max(rel_mat)+rel_non_tol, 4)\n",
    "\n",
    "def fn_matrix_rmse(mat):\n",
    "    return round(np.sqrt(np.mean(mat**2)), 4)\n",
    "\n",
    "def fn_quantify(vals_1, vals_2, vals_weights=None):            # not sure why its needed \n",
    "    return fn_quantify_matrix(vals_1, vals_2, vals_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function that quantify inadequacy or discrepancy\n",
    "# Params:\n",
    "# - iter_stats: output stats from iteration\n",
    "# - data_stats: output stats calculated from data\n",
    "\n",
    "def fn_quantify_inadequacy_stats(iter_stats, city_stats):\n",
    "    # If empty activity schedule is generated, return max distance\n",
    "    if iter_stats is None:\n",
    "        return fn_quantify(np.zeros(len(city_stats)), city_stats)\n",
    "    \n",
    "    # Merge iter_stats with city_stats based on the 'stats' column\n",
    "    joined_stats = pd.merge(iter_stats[['scope', 'type', 'variable', 'value']],\n",
    "                            city_stats.rename(columns={'value': 'observed_value'}),\n",
    "                            on=['scope', 'type', 'variable'], how='left')\n",
    "    \n",
    "    # Remove rows with missing observed values\n",
    "    joined_stats.dropna(subset=['observed_value'], inplace=True)\n",
    "    \n",
    "    return fn_quantify(joined_stats['value'], joined_stats['observed_value'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function that quantify inadequacy or discrepancy\n",
    "# Params:\n",
    "# - iter_stats: output stats from iteration\n",
    "# - data_stats: output stats calculated from data\n",
    "\n",
    "def fn_quantify_inadequacy_od(iter_stats):\n",
    "    # Get global settings object\n",
    "    setting_obj = shared.env.settings\n",
    "    \n",
    "    mat_weights = None\n",
    "    if \"weights\" in setting_obj[\"CITY_STATS\"]:\n",
    "        mat_weights = setting_obj[\"CITY_STATS\"][\"weights\"]\n",
    "    \n",
    "    # If empty activity schedule is generated (i.e., no trips at all), return max distance\n",
    "    if iter_stats is None:\n",
    "        od_dim = setting_obj[\"CITY_STATS\"][\"value\"].shape\n",
    "        return fn_quantify(np.zeros((od_dim[0], od_dim[1])), setting_obj[\"CITY_STATS\"][\"value\"], mat_weights)\n",
    "    \n",
    "    return fn_quantify(iter_stats, setting_obj[\"CITY_STATS\"][\"value\"], mat_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function that quantify inadequacy or discrepancy\n",
    "# Params:\n",
    "# - iter_stats: output stats from iteration\n",
    "# - data_stats: output stats calculated from data\n",
    "\n",
    "def fn_quantify_inadequacy_BCKUP(iter_stats):\n",
    "    # Get global settings object\n",
    "    setting_obj = shared.env.settings\n",
    "    \n",
    "    od_discrepancy = fn_quantify_inadequacy_od(iter_stats[\"od\"])\n",
    "    balance_discrepancy = 1\n",
    "    \n",
    "    if \"balance\" in iter_stats:\n",
    "        balance_discrepancy = 1 - fn_quantify_vector_correlation(iter_stats[\"balance\"][:,1], iter_stats[\"balance\"][:,2])\n",
    "    \n",
    "    # Calculate the value and base of the function and return as a dictionary\n",
    "    value = balance_discrepancy * abs(iter_stats[\"total_legs\"] - setting_obj[\"CITY_STATS\"][\"total_trips\"])\n",
    "    base = [od_discrepancy, iter_stats[\"total_legs\"], balance_discrepancy]\n",
    "    return {\"value\": value, \"base\": base}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_quantify_inadequacy(iter_stats):\n",
    "    # Get global settings object\n",
    "    setting_obj = shared.env['settings']\n",
    "    \n",
    "    od_discrepancy = fn_quantify_inadequacy_od(iter_stats['od'])\n",
    "    balance_discrepancy = 0\n",
    "    \n",
    "    if 'balance' in iter_stats:\n",
    "        balance_discrepancy = fn_quantify_vector(iter_stats['balance'][:,1], iter_stats['balance'][:,2])\n",
    "        \n",
    "    workers_work_error = 0\n",
    "    if 'workers_work' in iter_stats:\n",
    "        workers_work_error = iter_stats['workers_work']\n",
    "  # ## ----------- Error value ----------------\n",
    "  # simulated_num_legs <- iter_stats$total_legs\n",
    "  # observed_num_legs <- setting_obj$CITY_STATS$total_trips\n",
    "  # \n",
    "  # error_val <- (0.00001 + abs(simulated_num_legs-observed_num_legs)/observed_num_legs) * \n",
    "  #              (0.00001 + od_discrepancy) *\n",
    "  #              (1 + ((abs(simulated_num_legs-observed_num_legs)/observed_num_legs) - od_discrepancy)^2) ## DESC: [ NUM_LEGS_ERROR * OD_ERROR * DIFFERENCE_BETWEEN_THEM^2 ] both sides have equal multiplicative contribution to the loss/error. Minimum (optimum) is close to 0.\n",
    "  # \n",
    "  # error_base <- c()#list(tours_residual = abs(simulated_num_legs-observed_num_legs)/observed_num_legs, od_residual = od_discrepancy) # tours=iter_stats$total_legs\n",
    "  # \n",
    "  # if(exists(\"logger\", shared.env)){\n",
    "  #   log4r::info(shared.env$logger$LOGGER, paste0(\"ERROR-TERM: [Sim/Obs]Legs: \", simulated_num_legs, \"/\", observed_num_legs, \" (ABS-DIF: \", abs(simulated_num_legs-observed_num_legs)/observed_num_legs,\"); OD_DISCREPANCY: \", od_discrepancy, \"; OVERALL-ERROR: \", error_val))\n",
    "  # }\n",
    "  # \n",
    "  # ## ----------------------------------------\n",
    "    simulated_num_legs = iter_stats['total_legs']\n",
    "    observed_num_legs = setting_obj['CITY_STATS']['total_trips']\n",
    "    error_val = od_discrepancy/100 * (1 + balance_discrepancy) * (1 + workers_work_error)\n",
    "    error_base = []\n",
    "    \n",
    "    # if 'logger' in shared.env and 'LOGGER' in shared.env['logger']:\n",
    "    #     log4r.info(shared.env['logger']['LOGGER'], f\"ERROR-TERM: TOTAL-LEGS: {simulated_num_legs}/{observed_num_legs}; OD: {od_discrepancy}; BALANCE: {balance_discrepancy}; OVERALL-ERROR: {error_val}\")\n",
    "\n",
    "\n",
    "    if \"logger\" in shared.env and hasattr(shared.env.logger, \"LOGGER\") and shared.env.logger.LOGGER is not None:\n",
    "        shared.env.logger.LOGGER.info(f\"ERROR-TERM: TOTAL-LEGS: {simulated_num_legs}/{observed_num_legs}; OD: {od_discrepancy}; BALANCE: {balance_discrepancy}; OVERALL-ERROR: {error_val}\")\n",
    "    \n",
    "    return {'value': error_val, 'base': error_base}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function that take activity_schedule at input and return calculated output statistics\n",
    "# Params:\n",
    "# - /\n",
    "\n",
    "def fn_process_activities():\n",
    "    # Get global settings object\n",
    "    setting_obj = shared.env.settings\n",
    "\n",
    "    # Load activities\n",
    "    activity_schedule = pd.read_csv(setting_obj.ACTIVITY_FILE,\n",
    "                                    names=[\"person_id\",\"tour_no\",\"tourType\",\"stop_no\",\"stop_type\",\"stop_location\",\"stopZone\",\"stop_mode\",\"primary_stop\",\"arrival_time\",\"departure_time\",\"prev_stop_location\",\"prev_stopZone\",\"prev_stop_departure_time\",\"pid\"],\n",
    "                                    header=None)\n",
    "\n",
    "    # TEMPORAL: save file\n",
    "    activity_schedule.to_csv(setting_obj.OBJECT_PATH + setting_obj.FINGERPRINT + \"_activity_schedule_\" + str(time.time()) + \".csv\")\n",
    "\n",
    "    # Calculate outcomes (number of tours, trips, stops, VKT?, PKT / general + per mode + per activity)\n",
    "    #output_stats = fn_output_stat(activity_schedule)\n",
    "    #output_stats = fn_output_od(activity_schedule, setting_obj)\n",
    "    output_stats = fn_output_od_mode_balance(activity_schedule)\n",
    "\n",
    "    return output_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simulation function updates config file for simulation, runs the simmobility simulation (preday), reads the result and calculate measure of adequacy (similarity) of the result\n",
    "# Params:\n",
    "# - parameter dataframe (including name of parameter, model specification and declaration type (local or property))\n",
    "\n",
    "def fn_simulation(params):\n",
    "    # Update files with parameters\n",
    "    param_config_output = fn_simulation_config(params)\n",
    "\n",
    "    # Execute simulation\n",
    "    fn_simulation_call()\n",
    "\n",
    "    # Read and process activity schedules\n",
    "    outcome = fn_process_activities()\n",
    "\n",
    "    # Quantify (in)adequacy (discrepancy) of output statistics to the real world data (stats)\n",
    "    inadequacy = fn_quantify_inadequacy(outcome)\n",
    "    if shared.env.settings.MOCKING_MODE:\n",
    "        inadequacy[\"value\"] += np.random.normal(0, 3)\n",
    "\n",
    "    return {\"config_output\": param_config_output,\n",
    "            \"inadequacy\": inadequacy[\"value\"],\n",
    "            \"base_residuals\": inadequacy[\"base\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_perform_simulation(value, param_def):\n",
    "    # Transform param row into corresponding params tibble\n",
    "    param_def[\"value\"] = value\n",
    "\n",
    "    # Simulate\n",
    "    param_iter_run = fn_simulation(param_def)\n",
    "\n",
    "    # Warnings for particular param not being applied\n",
    "    # ... TODO\n",
    "\n",
    "    # Update the value pool\n",
    "    fn_update_value_pools(value, param_iter_run[\"base_residuals\"], param_def)\n",
    "\n",
    "    # Return outcome/inadequacy\n",
    "    return param_iter_run[\"inadequacy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_perform_simulation_only(value, param_def):\n",
    "    ## Get global settings object\n",
    "    setting_obj = shared.env.settings\n",
    "\n",
    "    # Transform param row into corresponding params tibble\n",
    "    param_def[\"value\"] = value\n",
    "\n",
    "    # Update files with parameters\n",
    "    param_config_output = fn_simulation_config(param_def)\n",
    "\n",
    "    # Execute simulation\n",
    "    fn_simulation_call()\n",
    "\n",
    "    # Load activities\n",
    "    activity_schedule = pd.read_csv(setting_obj.ACTIVITY_FILE,\n",
    "                                    names=[\"person_id\",\"tour_no\",\"tourType\",\"stop_no\",\"stop_type\",\"stop_location\",\"stopZone\",\"stop_mode\",\"primary_stop\",\"arrival_time\",\"departure_time\",\"prev_stop_location\",\"prev_stopZone\",\"prev_stop_departure_time\",\"pid\"],\n",
    "                                    header=None)\n",
    "\n",
    "    # Save file\n",
    "    activity_schedule.to_csv(setting_obj.OBJECT_PATH + setting_obj.FINGERPRINT + \"_activity_schedule_\" + str(int(time.time())) + \".csv\", index=False)\n",
    "\n",
    "    # Return activity_schedule\n",
    "    return activity_schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_scaleup_standard(l_sample, p_def):\n",
    "    def scale_column(c_i):\n",
    "        return (l_sample[:, c_i] * (p_def[c_i, 1] - p_def[c_i, 0])) + p_def[c_i, 0]\n",
    "    \n",
    "    return np.apply_along_axis(scale_column, 0, np.arange(l_sample.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fn_preday_sampling(vals=None param_space=None, sample_size=1, **kwargs):\n",
    "#     is_initial = True if (vals is None or param_space['space'].shape[0] == 0) else False\n",
    "    \n",
    "#     enabled_idx = np.arange(param_space['definition'].shape[0])\n",
    "#     if 'enabled' in param_space['definition'].dtype.names:\n",
    "#         enabled_idx = np.where(param_space['definition']['enabled'] == True)[0]\n",
    "#     dim_names = param_space['definition']['parameter']\n",
    "#     sample_dim = len(enabled_idx)\n",
    "    \n",
    "#     genereted_sample = np.tile(param_space['definition']['initial'], (sample_size, 1))\n",
    "    \n",
    "#     if is_initial:\n",
    "#         genereted_sample[:, enabled_idx] = (lhs.create_sample(param_space['definition'][enabled_idx], sample_size)).T\n",
    "#         genereted_sample = np.vstack([param_space['definition']['initial'], genereted_sample])\n",
    "#         genereted_sample = genereted_sample.astype(param_space['definition'].dtype)\n",
    "#         genereted_sample.dtype.names = param_space['definition'].dtype.names\n",
    "        \n",
    "#         return genereted_sample\n",
    "    \n",
    "#     if 'SAMPLING_OPTIMAL_TOLERANCE' in kwargs and kwargs['SAMPLING_OPTIMAL_TOLERANCE'] > 0:\n",
    "#         min_inadequacy = np.nanmin(param_space['space'][param_space['space'][shared.env['settings']['target_col']].notna()][shared.env['settings']['target_col']])\n",
    "#         vals = param_space['space'][param_space['space'][shared.env['settings']['target_col']] < (min_inadequacy + kwargs['SAMPLING_OPTIMAL_TOLERANCE'])][dim_names].dropna()\n",
    "        \n",
    "#         if vals.shape[0] == 1:\n",
    "#             vals = np.array([tuple(vals.values.tolist()[0])], dtype=param_space['definition'].dtype)\n",
    "#         else:\n",
    "#             vals = vals.to_records(index=False)\n",
    "    \n",
    "#     if vals.shape[0] == 1:\n",
    "#         vals = np.tile(vals[:, enabled_idx], (sample_size, 1))\n",
    "#     else:\n",
    "#         vals = vals[:, enabled_idx]\n",
    "    \n",
    "#     tours_intensity = [0.6, 0.6]  # high and low intensity\n",
    "#     mode_intensity = [0.5, 0.5]\n",
    "#     od_intensity = [0.5, 0.5]\n",
    "#     none_intensity = [0.2, 0.2]\n",
    "#     sampling_intensities = {\n",
    "#         'tours': tours_intensity,\n",
    "#         'mode': mode_intensity,\n",
    "#         'od': od_intensity,\n",
    "#         'none': none_intensity\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_preday_sampling(vals=None, param_space=None, sample_size=1, **kwargs):\n",
    "    # check if the given vals and param_space arguments are None or empty\n",
    "    print(\"-------------------------------vals------------------------------------------------\",vals)\n",
    "    print(\"-------------------------------param_space------------------------------------------------\",param_space)\n",
    "    print(\"-------------------------------sample_size------------------------------------------------\",sample_size)\n",
    "    is_initial = True if (vals is None or param_space['space'].shape[0] == 0) else False\n",
    "    \n",
    "    # get the indexes of enabled parameters\n",
    "    enabled_idx = np.arange(param_space['definition'].shape[0])\n",
    "    if 'enabled' in param_space['definition'].dtype.names:\n",
    "        enabled_idx = np.where(param_space['definition']['enabled'] == True)[0]\n",
    "    \n",
    "    # get the names of enabled parameters\n",
    "    dim_names = param_space['definition']['parameter']\n",
    "    \n",
    "    # get the number of enabled parameters\n",
    "    sample_dim = len(enabled_idx)\n",
    "    \n",
    "    # create the generated sample numpy array\n",
    "    genereted_sample = np.tile(param_space['definition']['initial'], (sample_size, 1))\n",
    "    \n",
    "    # if the given vals and param_space arguments are None or empty, generate a new sample\n",
    "    if is_initial:\n",
    "        genereted_sample[:, enabled_idx] = (lhs.create_sample(param_space['definition'][enabled_idx], sample_size)).T\n",
    "        genereted_sample = np.vstack([param_space['definition']['initial'], genereted_sample])\n",
    "        genereted_sample = genereted_sample.astype(param_space['definition'].dtype)\n",
    "        genereted_sample.dtype.names = param_space['definition'].dtype.names\n",
    "        \n",
    "        return genereted_sample\n",
    "    \n",
    "    # if the SAMPLING_OPTIMAL_TOLERANCE keyword argument is given and its value is greater than 0,\n",
    "    # find the minimum value of target_col column in the param_space array and \n",
    "    # filter the rows where the target_col value is less than the sum of the minimum value and SAMPLING_OPTIMAL_TOLERANCE\n",
    "    if 'SAMPLING_OPTIMAL_TOLERANCE' in kwargs and kwargs['SAMPLING_OPTIMAL_TOLERANCE'] > 0:\n",
    "        min_inadequacy = np.nanmin(param_space['space'][param_space['space'][shared.env['settings']['target_col']].notna()][shared.env['settings']['target_col']])\n",
    "        vals = param_space['space'][param_space['space'][shared.env['settings']['target_col']] < (min_inadequacy + kwargs['SAMPLING_OPTIMAL_TOLERANCE'])][dim_names].dropna()\n",
    "        \n",
    "        if vals.shape[0] == 1:\n",
    "            vals = np.array([tuple(vals.values.tolist()[0])], dtype=param_space['definition'].dtype)\n",
    "        else:\n",
    "            vals = vals.to_records(index=False)\n",
    "    \n",
    "    # if there is only one row in the vals array, tile it to create a new vals array with the same shape as the generated sample\n",
    "    if vals.shape[0] == 1:\n",
    "        vals = np.tile(vals[:, enabled_idx], (sample_size, 1))\n",
    "    else:\n",
    "        vals = vals[:, enabled_idx]\n",
    "    \n",
    "    # set the intensities for each sampling method\n",
    "    tours_intensity = [0.6, 0.6]  # high and low intensity\n",
    "    mode_intensity = [0.5, 0.5]\n",
    "    od_intensity = [0.5, 0.5]\n",
    "    none_intensity = [0.2, 0.2]\n",
    "    sampling_intensities = {\n",
    "        'tours': tours_intensity,\n",
    "        'mode': mode_intensity,\n",
    "        'od': od_intensity,\n",
    "        'none': none_intensity\n",
    "    }\n",
    "\n",
    "    # Enable only parameters in the active subspace\n",
    "    enabled_idx = np.where(param_space['active'])[0]\n",
    "    dim_names = param_space['names'][enabled_idx]\n",
    "    sample_dim = len(enabled_idx)\n",
    "    \n",
    "    # Set up intensity values for each parameter\n",
    "    sampling_intensities = shared_env['settings']['SAMPLING_INTENSITY']\n",
    "    sampling_intensity = {}\n",
    "    for s_intensity in sampling_intensities:\n",
    "        \n",
    "        if (shared_env['param_space']['residual_pool'][s_intensity] is None \n",
    "            or len(shared_env['param_space']['residual_pool'][s_intensity]) < 2):\n",
    "            sampling_intensity[s_intensity] = sampling_intensities[s_intensity][0]\n",
    "            continue\n",
    "        \n",
    "        tail_diff = np.abs(np.diff(shared_env['param_space']['residual_pool'][s_intensity])[-3:])\n",
    "        thrs = shared_env['settings']['EPSILON'][s_intensity]\n",
    "        \n",
    "        if not np.any(tail_diff > thrs['focused_sampling']):\n",
    "            sampling_intensity[s_intensity] = sampling_intensities[s_intensity][0] \n",
    "        elif not np.any(tail_diff > thrs['spread_sampling']):\n",
    "            sampling_intensity[s_intensity] = sampling_intensities[s_intensity][1]\n",
    "        else:\n",
    "            sampling_intensity[s_intensity] = sampling_intensities[s_intensity][0]\n",
    "            \n",
    "    # Generate sample for each parameter\n",
    "    generated_sample = np.zeros((sample_size, len(dim_names)))\n",
    "    for i, c_i in enumerate(enabled_idx):\n",
    "        \n",
    "        p_def = param_space['definition'][c_i]\n",
    "        p_intensity = sampling_intensity[p_def['direct_influence']]\n",
    "        p_replaced_size = int(np.ceil(sample_size * p_intensity))\n",
    "        p_current = vals[np.random.choice(np.arange(vals.shape[0])), c_i]\n",
    "        \n",
    "        p_lower_bound = p_def['lower_limit']\n",
    "        p_upper_bound = p_def['upper_limit']\n",
    "        p_tunnel_bound = 2\n",
    "        if 'tunnel_width' in p_def:\n",
    "            p_tunnel_bound = p_def['tunnel_width']\n",
    "            p_lower_bound = p_current - p_tunnel_bound\n",
    "            p_upper_bound = p_current + p_tunnel_bound\n",
    "        \n",
    "        p_sample = np.repeat(p_current, sample_size)\n",
    "        p_sample_replace = np.random.choice(np.arange(sample_size), p_replaced_size, replace=False)\n",
    "        \n",
    "        # Uniform sampling\n",
    "        # p_sample[p_sample_replace] = np.random.uniform(p_lower_bound, p_upper_bound, p_replaced_size)\n",
    "        \n",
    "        # Normal around the optimal sampling\n",
    "        p_sample[p_sample_replace] = np.random.normal(p_current, p_tunnel_bound, p_replaced_size)\n",
    "        \n",
    "        generated_sample[:, i] = p_sample\n",
    "        \n",
    "    # Add initial parameter values to the generated sample\n",
    "    if not shared_env['settings'].get('IMPUTED_INITIAL_VALUE_SET', False):\n",
    "        if not np.allclose(param_space['space'][0, :len(enabled_idx)], \n",
    "                            np.array([p['initial'] for p in param_space['definition'][enabled_idx]])):\n",
    "            init_param_values = np.array([p['initial'] for p in param_space['definition']])\n",
    "            generated_sample = np.vstack((init_param_values, generated_sample))\n",
    "        shared_env['settings']['IMPUTED_INITIAL_VALUE_SET'] = True\n",
    "    \n",
    "    return generated_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to update the value pool that will be used in sampling\n",
    "\n",
    "def update_value_pools(params, base_residuals, param_def):\n",
    "    # Get global settings object\n",
    "    setting_obj = shared.env['settings']\n",
    "  \n",
    "    if setting_obj['MOCKING_MODE']:\n",
    "        base_residuals = base_residuals + np.random.normal(size=len(base_residuals))\n",
    "  \n",
    "    residual_object = shared.env['param_space']['residual_pool']\n",
    "    residual_components = set(residual_object.keys()) - set(['none'])\n",
    "  \n",
    "    for comp in residual_components:\n",
    "        shared.env['param_space']['residual_pool'][comp] = np.concatenate((np.array(residual_object[comp], dtype=np.float64), np.array(base_residuals[comp], dtype=np.float64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function that loads definition of param data.frame with provided values (if any)\n",
    "def fn_load_params_space_definition(filepath, omit = [], sys_modules = [\"preday\"], tunnel_constraint = True, update_initial = False):\n",
    "    no_tours_models = [\"dpb\",\"dps\",\"dpt\",\"nte\",\"ntw\",\"nto\",\"nts\",\"isg\"]\n",
    "    mode_balance_models = [\"tme\",\"tmw\",\"stmd\"]\n",
    "    low_priority_models = [\"tws\",\"sttd\",\"ttde\",\"ttdw\",\"ttdo\",\"itd\"]\n",
    "  \n",
    "    # Read file content\n",
    "    file_content = pd.read_csv(filepath)\n",
    "    file_content = file_content[file_content['module'].isin(sys_modules) & file_content['include']]\n",
    "    file_content[['param_name', 'value']] = file_content['param'].str.split('=', expand=True)\n",
    "    file_content['declaration'] = file_content['param'].str.extract('(.+?(?=beta|cons))')\n",
    "    file_content['param_name_left_hand'] = file_content['param_name']\n",
    "    file_content['param_name'] = file_content['param_name'].str.replace('(local |bundled_variables.)', '').str.strip()\n",
    "    file_content['parameter'] = file_content['model'] + '_' + file_content['param_name']\n",
    "    file_content['raw_value'] = file_content['value'].str.strip()\n",
    "    file_content['value'] = file_content['value'].str.replace(' ','').astype(float)\n",
    "    file_content['direct_influence'] = np.where(file_content['model'].isin(no_tours_models), 'tours',\n",
    "                                                np.where(file_content['model'].isin(mode_balance_models), 'mode', 'none'))\n",
    "    file_content['enabled'] = (file_content['lower_limit'] != file_content['upper_limit'])\n",
    "  \n",
    "    # If enabled, set initial value as lower limit\n",
    "    if update_initial:\n",
    "        file_content['initial'] = np.where(file_content['enabled'], file_content['value'], file_content['lower_limit'])\n",
    "  \n",
    "    # Omit specified columns\n",
    "    if isinstance(omit, str):\n",
    "        omit_col_name = omit\n",
    "        omit = file_content.loc[file_content[omit]].parameter.tolist()\n",
    "        file_content = file_content.drop(columns=omit_col_name)\n",
    "    else:\n",
    "        file_content = file_content[~file_content['parameter'].isin(omit)]\n",
    "  \n",
    "    # Apply tunnel constraint (lower and upper bounds around current value +/- t_width)\n",
    "    if tunnel_constraint:\n",
    "        t_width = 2.0\n",
    "        t_restricted_width = 0.05\n",
    "        file_content['lower_limit'] = np.where(~file_content['enabled'], file_content['initial'],\n",
    "                                               np.where(file_content['param_name'].str.contains('logsum'),\n",
    "                                                        file_content['initial'] - t_restricted_width,\n",
    "                                                        file_content['initial'] - t_width))\n",
    "        file_content['upper_limit'] = np.where(~file_content['enabled'], file_content['initial'],\n",
    "                                               np.where(file_content['param_name'].str.contains('logsum'),\n",
    "                                                        file_content['initial'] + t_restricted_width,\n",
    "                                                        file_content['initial'] + t_width))\n",
    "        file_content['tunnel_width'] = t_width\n",
    "  \n",
    "    # Return output as dictionary\n",
    "    definition = file_content.drop(columns='value').assign(changed=~file_content['parameter'].isin(omit),\n",
    "                                                            init_lower_limit=file_content['lower_limit'],\n",
    "                                                            init_upper_limit=file_content['upper_limit'])\n",
    "    space = file_content[['value']].T.rename(columns=lambda x: file_content.loc[x, 'parameter']).iloc[:, 1:]\n",
    "    value_pool = {col: [] for col in definition['parameter']}\n",
    "    residual_pool = {'tours': [], 'mode': [], 'od': []}\n",
    "  \n",
    "    return {'definition': definition,\n",
    "            'space': space,\n",
    "            'value_pool': value_pool,\n",
    "            'residual_pool':residual_pool}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
